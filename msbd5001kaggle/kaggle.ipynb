{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.decomposition import PCA\n",
    "import math \n",
    "\n",
    "def is_number(s):\n",
    "# to judge whether the feature value is integer\n",
    "    try:\n",
    "        float(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        import unicodedata\n",
    "        unicodedata.numeric(s)\n",
    "        return True\n",
    "    except (TypeError, ValueError):\n",
    "        pass\n",
    "\n",
    "    return False\n",
    "\n",
    "def feature_eng(data):\n",
    "    std = StandardScaler()\n",
    "    #data = data.replace('none', 0)\n",
    "    data = data.fillna(0)\n",
    "    #print(data.penalty)\n",
    "#if feature value is not interger, factorize it\n",
    "    for name in data.columns:\n",
    "        if (name == 'penalty'):\n",
    "            data[name] = pd.factorize(data[name].values)[0]+1\n",
    "        if (name =='alpha'):\n",
    "            temp = data[name].map(lambda x:-math.log10(x))\n",
    "            data[name] = temp\n",
    "#             print('ssssss')\n",
    "\n",
    "#new feature\n",
    "    data['max_iter*n_samples'] = data['max_iter']* data['n_samples']* data['n_classes'] \n",
    "    \n",
    "    data['tt'] = pd.Series(list(lab))\n",
    "    data = data.drop(['id', 'flip_y'], axis = 1)\n",
    "    data['n_jobs'] = 1/data['n_jobs']\n",
    "#     data= normalize(data, norm='l1')\n",
    "    data = data.apply(lambda x: (x - np.min(x)) / (np.max(x) - np.min(x)))\n",
    "#     data = (data - data.mean()) / (data.std())\n",
    "    #print(data)\n",
    "    # df = data\n",
    "    # data = (df - df.min()) / (df.max() - df.min())\n",
    "#     num_pca = 8\n",
    "#     pca = PCA(n_components=num_pca)\n",
    "#     pca.fit(data)\n",
    "#     columns = ['pca_%i' % i for i in range(num_pca)]\n",
    "#     data = pd.DataFrame(pca.transform(data), columns=columns, index=data.index)\n",
    "    \n",
    "\n",
    "    #print(df_pca.head())\n",
    "    #print(data)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_lab = pd.read_csv('./data/train_data1.csv').T\n",
    "lab = train_lab[1].append(pd.read_csv(\"./data/res_v19.csv\")['time'][1:])\n",
    "lab.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     alpha  l1_ratio  max_iter  n_classes  n_clusters_per_class  n_features  \\\n",
      "0      1.0  0.254865  0.350838      0.250              0.333333    0.115242   \n",
      "1      0.5  0.784872  0.530726      0.250              1.000000    0.139671   \n",
      "2      0.0  0.807567  0.541899      0.000              1.000000    0.577801   \n",
      "3      0.5  0.468191  0.811173      0.500              1.000000    0.107807   \n",
      "4      0.5  0.368666  0.071508      0.750              1.000000    0.283590   \n",
      "5      0.5  0.323364  0.986592      0.250              0.000000    0.398832   \n",
      "6      1.0  0.823429  0.922905      0.375              0.000000    0.308550   \n",
      "7      1.0  0.228291  0.356425      0.875              1.000000    0.332979   \n",
      "8      0.0  0.832950  0.229050      0.000              0.666667    0.574084   \n",
      "9      0.5  0.185979  0.454749      0.875              1.000000    0.368030   \n",
      "10     1.0  0.960458  0.420112      0.625              0.666667    0.497610   \n",
      "11     1.0  0.592793  0.174302      1.000              0.000000    0.280935   \n",
      "12     0.5  0.402327  0.575419      0.125              1.000000    0.045141   \n",
      "13     0.0  0.498036  0.631285      0.875              0.666667    0.669145   \n",
      "14     1.0  0.698527  0.316201      0.375              0.333333    0.520446   \n",
      "15     1.0  0.117015  0.000000      0.125              0.333333    0.245884   \n",
      "16     0.0  0.870289  0.656983      0.125              1.000000    0.619225   \n",
      "17     0.0  0.664782  0.688268      0.875              0.333333    0.052576   \n",
      "18     0.5  0.414339  0.181006      0.125              1.000000    0.473181   \n",
      "19     1.0  0.793406  0.292737      0.250              0.666667    0.407329   \n",
      "20     0.5  0.785859  0.122905      0.500              0.666667    0.009559   \n",
      "21     0.5  0.700171  0.197765      0.125              1.000000    0.284652   \n",
      "22     1.0  0.722013  0.122905      0.750              0.000000    0.240042   \n",
      "23     0.0  0.105128  0.226816      1.000              1.000000    0.266065   \n",
      "24     1.0  0.501364  0.661453      0.125              0.000000    0.156665   \n",
      "25     0.5  0.659668  0.858101      0.500              0.333333    0.190122   \n",
      "26     0.5  0.280505  0.972067      0.625              0.333333    0.071694   \n",
      "27     1.0  0.945565  0.737430      0.500              0.666667    0.010621   \n",
      "28     0.5  0.183507  0.030168      1.000              0.000000    0.451407   \n",
      "29     0.0  0.668081  0.830168      0.875              1.000000    0.601699   \n",
      "..     ...       ...       ...        ...                   ...         ...   \n",
      "470    0.5  0.700679  0.001117      0.125              0.333333    0.756771   \n",
      "471    1.0  0.801233  0.286034      0.125              1.000000    0.420605   \n",
      "472    1.0  0.246357  0.517318      0.250              0.000000    0.420074   \n",
      "473    0.5  0.116625  0.503911      0.000              1.000000    0.215613   \n",
      "474    0.5  0.333930  0.088268      0.500              0.333333    0.751992   \n",
      "475    0.5  0.088704  0.981006      0.125              0.333333    0.675518   \n",
      "476    0.0  0.137120  0.743017      0.375              0.666667    0.738715   \n",
      "477    1.0  0.721837  0.750838      0.625              0.333333    0.709506   \n",
      "478    1.0  0.303538  0.116201      0.875              0.333333    0.864047   \n",
      "479    1.0  0.339942  0.075978      0.250              0.333333    0.474774   \n",
      "480    0.5  0.706688  0.426816      0.750              0.666667    0.601168   \n",
      "481    1.0  0.011107  0.690503      0.500              0.666667    0.800319   \n",
      "482    0.5  0.795741  0.781006      0.625              0.666667    0.707382   \n",
      "483    0.0  0.631458  0.216760      0.500              0.333333    0.207116   \n",
      "484    0.0  0.032899  0.418994      0.750              0.666667    0.141795   \n",
      "485    1.0  0.922924  0.400000      0.625              0.000000    0.678173   \n",
      "486    1.0  0.456788  0.306145      0.375              0.666667    0.928306   \n",
      "487    0.5  0.698027  0.948603      0.750              1.000000    0.479554   \n",
      "488    0.0  0.051657  0.484916      0.625              0.000000    0.106745   \n",
      "489    0.0  0.121967  0.207821      1.000              0.000000    0.195433   \n",
      "490    1.0  0.813228  0.347486      0.750              0.666667    0.404673   \n",
      "491    1.0  0.367452  0.163128      0.500              0.000000    0.126925   \n",
      "492    0.0  0.577639  0.798883      0.250              1.000000    0.568242   \n",
      "493    1.0  0.097088  0.920670      0.375              0.333333    0.614445   \n",
      "494    1.0  0.406541  0.563128      0.750              0.666667    0.724376   \n",
      "495    1.0  0.429665  0.518436      0.375              1.000000    0.487520   \n",
      "496    0.5  0.427339  0.569832      1.000              0.666667    0.443972   \n",
      "497    1.0  0.734118  0.217877      0.375              0.333333    0.566649   \n",
      "498    1.0  0.664348  0.778771      1.000              0.333333    0.491237   \n",
      "499    0.0  0.677377  0.185475      0.250              1.000000    0.334041   \n",
      "\n",
      "     n_informative    n_jobs  n_samples   penalty  random_state     scale  \\\n",
      "0         0.285714  0.066667   0.522451  0.000000         0.475  0.231258   \n",
      "1         0.285714  1.000000   0.364501  0.333333         0.569  0.540605   \n",
      "2         0.142857  0.466667   0.173270  0.000000         0.529  0.167707   \n",
      "3         0.285714  0.200000   0.410460  0.000000         0.103  0.821919   \n",
      "4         0.857143  0.466667   0.061278  0.666667         0.418  0.956905   \n",
      "5         0.142857  0.200000   0.684628  0.333333         0.094  0.309973   \n",
      "6         0.142857  0.466667   0.426836  0.000000         0.532  0.984626   \n",
      "7         0.428571  0.066667   0.304807  0.333333         0.143  0.540625   \n",
      "8         0.571429  0.066667   0.210248  0.333333         0.085  0.535922   \n",
      "9         0.714286  0.066667   0.676175  1.000000         0.574  0.748522   \n",
      "10        0.428571  0.066667   0.569466  0.000000         0.535  0.734378   \n",
      "11        0.857143  0.466667   0.401479  1.000000         0.960  0.828513   \n",
      "12        0.571429  0.200000   0.278922  0.666667         0.460  0.115478   \n",
      "13        0.428571  0.066667   0.211305  1.000000         0.503  0.469267   \n",
      "14        0.571429  0.466667   0.426307  1.000000         0.066  0.429411   \n",
      "15        0.428571  0.466667   0.005811  0.000000         0.288  0.222055   \n",
      "16        0.285714  1.000000   0.202853  0.000000         0.947  0.860845   \n",
      "17        0.571429  0.466667   0.001585  0.666667         0.259  0.767255   \n",
      "18        0.285714  0.200000   0.288431  0.666667         0.895  0.525018   \n",
      "19        0.857143  0.466667   0.608030  0.666667         0.736  0.900594   \n",
      "20        0.428571  0.200000   0.064448  0.000000         0.101  0.197428   \n",
      "21        0.142857  0.066667   0.124670  0.666667         0.443  0.164775   \n",
      "22        0.571429  0.466667   0.400951  0.000000         0.445  0.426832   \n",
      "23        0.428571  0.066667   0.319070  0.000000         0.446  0.252352   \n",
      "24        0.142857  0.466667   0.418912  0.333333         0.254  0.903265   \n",
      "25        0.571429  1.000000   0.038563  0.000000         0.499  0.520964   \n",
      "26        0.571429  0.466667   0.172213  0.666667         0.987  0.815007   \n",
      "27        0.857143  0.200000   0.255679  1.000000         0.834  0.387024   \n",
      "28        0.571429  0.066667   0.604332  0.000000         0.854  0.181081   \n",
      "29        0.857143  0.466667   0.163233  0.000000         0.531  0.556219   \n",
      "..             ...       ...        ...       ...           ...       ...   \n",
      "470       0.571429  0.000000   0.534073  0.000000         0.727  0.532929   \n",
      "471       0.714286  0.466667   0.486529  1.000000         0.841  0.794678   \n",
      "472       0.571429  0.466667   0.797147  0.333333         0.928  0.783756   \n",
      "473       0.285714  0.200000   0.322768  1.000000         0.328  0.578118   \n",
      "474       0.285714  0.200000   0.966719  0.333333         0.809  0.799303   \n",
      "475       0.714286  1.000000   0.511886  1.000000         0.495  0.891332   \n",
      "476       0.857143  0.000000   0.380349  0.000000         0.056  0.199228   \n",
      "477       0.714286  0.466667   0.354992  1.000000         0.370  0.365509   \n",
      "478       0.857143  0.200000   0.212361  0.666667         0.122  0.657719   \n",
      "479       0.285714  1.000000   0.759641  1.000000         0.463  0.553675   \n",
      "480       0.428571  0.000000   0.838880  0.666667         0.103  0.031123   \n",
      "481       0.285714  0.200000   0.797147  0.000000         0.754  0.217545   \n",
      "482       0.857143  0.200000   0.880613  1.000000         0.862  0.071360   \n",
      "483       0.285714  0.066667   0.347596  0.333333         0.007  0.217228   \n",
      "484       1.000000  0.466667   0.080824  0.333333         0.850  0.882120   \n",
      "485       0.285714  0.200000   0.995246  0.000000         0.594  0.812358   \n",
      "486       0.285714  0.066667   0.586371  0.333333         0.933  0.816564   \n",
      "487       0.857143  0.066667   0.033281  0.000000         0.471  0.650343   \n",
      "488       0.285714  0.466667   0.599049  0.666667         0.227  0.332130   \n",
      "489       0.428571  0.466667   0.470681  0.000000         0.444  0.705263   \n",
      "490       0.428571  0.200000   0.225568  0.666667         0.177  0.753262   \n",
      "491       0.428571  1.000000   0.632330  1.000000         0.959  0.621608   \n",
      "492       0.857143  0.066667   0.526149  0.000000         0.134  0.874698   \n",
      "493       0.142857  0.200000   0.582673  0.333333         0.864  0.724294   \n",
      "494       0.714286  1.000000   0.396197  1.000000         0.137  0.709795   \n",
      "495       0.428571  0.466667   0.818806  0.000000         0.519  0.265421   \n",
      "496       1.000000  1.000000   0.735869  0.333333         0.019  0.640931   \n",
      "497       0.142857  0.000000   0.162705  1.000000         0.780  0.496452   \n",
      "498       0.428571  0.200000   0.067089  0.000000         0.150  0.677348   \n",
      "499       0.714286  1.000000   0.008452  1.000000         0.432  0.351248   \n",
      "\n",
      "     max_iter*n_samples        tt  \n",
      "0              0.107958  0.020258  \n",
      "1              0.108566  0.010712  \n",
      "2              0.028413  0.079133  \n",
      "3              0.262173  0.010512  \n",
      "4              0.015403  0.022772  \n",
      "5              0.331450  0.012409  \n",
      "6              0.253417  0.154635  \n",
      "7              0.153682  0.059878  \n",
      "8              0.016505  0.049572  \n",
      "9              0.381631  0.021368  \n",
      "10             0.237193  0.044293  \n",
      "11             0.132852  0.022205  \n",
      "12             0.068453  0.025101  \n",
      "13             0.180018  0.014400  \n",
      "14             0.103964  0.031400  \n",
      "15             0.000000  0.041995  \n",
      "16             0.058701  0.002111  \n",
      "17             0.038298  0.042713  \n",
      "18             0.029033  0.010201  \n",
      "19             0.108564  0.023220  \n",
      "20             0.015109  0.103314  \n",
      "21             0.015020  0.002076  \n",
      "22             0.086592  0.008433  \n",
      "23             0.128420  0.014895  \n",
      "24             0.110712  0.010774  \n",
      "25             0.052690  0.063577  \n",
      "26             0.173687  0.013994  \n",
      "27             0.159879  0.055099  \n",
      "28             0.095888  0.008249  \n",
      "29             0.186568  0.013996  \n",
      "..                  ...       ...  \n",
      "470            0.018920  0.014305  \n",
      "471            0.064534  0.015752  \n",
      "472            0.218592  0.200410  \n",
      "473            0.045644  0.000015  \n",
      "474            0.125593  0.137551  \n",
      "475            0.188503  0.110769  \n",
      "476            0.188668  0.022940  \n",
      "477            0.251626  0.080063  \n",
      "478            0.054576  0.077261  \n",
      "479            0.061649  0.035301  \n",
      "480            0.394670  0.164359  \n",
      "481            0.419573  0.115006  \n",
      "482            0.598846  0.132829  \n",
      "483            0.079747  0.011881  \n",
      "484            0.056529  0.023230  \n",
      "485            0.385761  0.084349  \n",
      "486            0.136090  0.124801  \n",
      "487            0.073123  0.007788  \n",
      "488            0.278917  0.129013  \n",
      "489            0.171415  0.020591  \n",
      "490            0.103655  0.066339  \n",
      "491            0.115304  0.022948  \n",
      "492            0.215159  0.033919  \n",
      "493            0.335696  0.327228  \n",
      "494            0.247961  0.151277  \n",
      "495            0.281288  0.080552  \n",
      "496            0.552332  1.000000  \n",
      "497            0.034748  0.000000  \n",
      "498            0.107931  0.014250  \n",
      "499            0.005483  0.004659  \n",
      "\n",
      "[500 rows x 14 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nick/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/nick/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/nick/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py:6201: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=True'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass sort=False\n",
      "\n",
      "  sort=sort)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn import model_selection\n",
    "from sklearn import tree\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import svm\n",
    "from sklearn import neighbors\n",
    "from sklearn import linear_model\n",
    "from sklearn import neural_network\n",
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_data = pd.read_csv(\"./data/train.csv\")\n",
    "test_data = pd.read_csv(\"./data/test.csv\")\n",
    "test_label = pd.read_csv(\"./data/gt.csv\")\n",
    "train_len = len(train_data)\n",
    "\n",
    "train_data['n_jobs'][train_data['n_jobs'] == -1] = 8\n",
    "test_data['n_jobs'][test_data['n_jobs'] == -1] = 16\n",
    "\n",
    "all_proc = feature_eng(train_data.iloc[:, :-1].append(test_data.iloc[:, 1:], ignore_index=True))\n",
    "train = all_proc[:]\n",
    "test = all_proc[train_len:]\n",
    "label = train_data.time.append(test_label.iloc[:,1])\n",
    "# train = all_proc[:train_len]\n",
    "# test = all_proc[train_len:]\n",
    "# label = train_data.time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBRegressor(learning_rate= 0.1, n_estimators=1000, \n",
    "                    max_depth=3, min_child_weight= 6, seed= 1,\n",
    "                    subsample= 0.8, colsample_bytree= 0.4, \n",
    "                    gamma=0, reg_alpha=0.05, reg_lambda=0.1)\n",
    "model1 = tree.DecisionTreeRegressor()\n",
    "model2 = LinearRegression()\n",
    "model3 = svm.SVR()\n",
    "model4 = neighbors.KNeighborsRegressor()\n",
    "model5 = linear_model.SGDClassifier()\n",
    "model6 = neural_network.MLPRegressor(hidden_layer_sizes = (50,25, 16 ,1))\n",
    "model7 = ensemble.GradientBoostingRegressor(n_estimators=1200)\n",
    "model8 = ensemble.ExtraTreesRegressor(n_estimators=500)\n",
    "model9 = ensemble.RandomForestRegressor(n_estimators=2000, random_state = 0)\n",
    "model10 = ensemble.AdaBoostRegressor(model7)\n",
    "model11 = ensemble.BaggingRegressor(model7)\n",
    "param_grid={\"hidden_layer_sizes\": [(100,90,80,70,60,50), (90,90,80,70,60,50)]}\n",
    "model12 = GridSearchCV(model6, param_grid=param_grid,verbose=2)\n",
    "\n",
    "\n",
    "def gen(model):\n",
    "    model.fit(train, label)\n",
    "    predicted = model.predict(test)\n",
    "\n",
    "    df = pd.DataFrame({'id':{},'time':{}})\n",
    "    df.id = test_data.id\n",
    "    df.time = pd.Series(predicted)\n",
    "    df.time[df.time<0] = 0\n",
    "    df.to_csv('./data/res_v21.csv', index = 0)\n",
    "\n",
    "\n",
    "def crossV(model):\n",
    "  kf = model_selection.KFold(n_splits=5, shuffle=False, random_state=1)\n",
    "  scores = model_selection.cross_val_score(model, train, label, scoring='mean_squared_error', cv=kf)\n",
    "\n",
    "  print(scores)\n",
    "  print(scores.mean())\n",
    "\n",
    "def plot_importance(forest):\n",
    "    forest.fit(train, label)\n",
    "    importances = forest.feature_importances_\n",
    "    std = np.std([tree.feature_importances_ for tree in forest.estimators_],\n",
    "                 axis=0)\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "\n",
    "    # Print the feature ranking\n",
    "    print(\"Feature ranking:\")\n",
    "\n",
    "    for f in range(train.shape[1]):\n",
    "        print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n",
    "\n",
    "    # Plot the feature importances of the forest\n",
    "    plt.figure()\n",
    "    plt.title(\"Feature importances\")\n",
    "    plt.bar(range(train.shape[1]), importances[indices],\n",
    "           color=\"r\", yerr=std[indices], align=\"center\")\n",
    "    plt.xticks(range(train.shape[1]), indices)\n",
    "    plt.xlim([-1, train.shape[1]])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature ranking:\n",
      "1. feature 12 (0.278326)\n",
      "2. feature 7 (0.159609)\n",
      "3. feature 13 (0.128266)\n",
      "4. feature 9 (0.117831)\n",
      "5. feature 5 (0.091438)\n",
      "6. feature 2 (0.033730)\n",
      "7. feature 8 (0.027424)\n",
      "8. feature 1 (0.025733)\n",
      "9. feature 10 (0.025353)\n",
      "10. feature 4 (0.025044)\n",
      "11. feature 3 (0.023437)\n",
      "12. feature 6 (0.022816)\n",
      "13. feature 11 (0.021763)\n",
      "14. feature 0 (0.019230)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# crossV(model7)\n",
    "# gen(model)\n",
    "plot_importance(model8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossV(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen(model7)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
